# üéØ Miss√£o

Miss√£o do projeto √© desenvolver modelos preditivos capazes de prever quais clientes t√™m maior chance de cancelar seus servi√ßos.
A empresa quer antecipar o problema da evas√£o, e cabe a voc√™ construir um pipeline robusto para essa etapa inicial de modelagem.

# An√°lise e Predi√ß√£o de Churn em Telecomunica√ß√µes (Parte 2)

Este projeto tem como objetivo desenvolver modelos preditivos para identificar clientes com alta probabilidade de cancelar seus servi√ßos (churn) em uma empresa de telecomunica√ß√µes. A miss√£o √© construir um pipeline robusto que inclua desde a prepara√ß√£o dos dados at√© a avalia√ß√£o e interpreta√ß√£o dos modelos.

## üóÉÔ∏è Bibliotecas Utilizadas e suas Funcionalidades

O projeto utiliza as seguintes bibliotecas Python, com foco em manipula√ß√£o de dados, visualiza√ß√£o e Machine Learning.

  * **Pandas**: A "canivete su√≠√ßo" para dados\! √â a biblioteca mais importante para organizar e mexer com dados em formato de tabelas (que chamamos de DataFrames).

      * **Carregando e organizando os dados**: O notebook come√ßa lendo os dados de um arquivo JSON. As informa√ß√µes v√™m bagun√ßadas, com v√°rios dados dentro de outros. O Pandas entra em a√ß√£o para arrumar tudo.
        ```python
        # Carrega os dados de um arquivo JSON em uma URL
        dados = pd.read_json('https://path/to/data.json')
        # "Achata" as colunas com dicion√°rios aninhados, criando colunas novas e arrumadas
        dados_normalizados = pd.json_normalize(dados.to_dict('records'))
        ```

  * **Numpy**: A base para c√°lculos com n√∫meros. Ela trabalha com arrays (matrizes) de forma super r√°pida e eficiente. Muitas outras bibliotecas, como o Scikit-learn, usam o Numpy por baixo dos panos.

      * **Exemplo popular de uso**: O `numpy` √© fundamental para opera√ß√µes matem√°ticas complexas, como as que acontecem dentro dos algoritmos de Machine Learning.

  * **Matplotlib.pyplot e Seaborn**: Para "desenhar" gr√°ficos e visualizar os dados. O `Seaborn` √© um complemento do `Matplotlib` que deixa os gr√°ficos mais bonitos e f√°ceis de fazer.

      * **Visualizando os resultados**: Uma matriz de confus√£o √© um gr√°fico importante para entender como o modelo acertou e errou.
        ```python
        # Plota a matriz de confus√£o
        sns.heatmap(matriz_confusao, annot=True, fmt='d', cmap='Blues')
        plt.show()
        ```

  * **Scikit-learn (sklearn)**: A "caixa de ferramentas" do Machine Learning. Com ela, fazemos tudo: preparamos os dados, treinamos os modelos e vemos o quanto eles acertaram.

      * **Preparando os dados para o modelo**:
          * `train_test_split`: Separa os dados em dois grupos: um para o modelo aprender (treino) e outro para testar se ele aprendeu direito (teste).
          * `StandardScaler`: Ajuda a padronizar os n√∫meros. Por exemplo, se uma coluna tem valores de 1 a 10 e outra de 1 a 1000, o `StandardScaler` deixa ambas na mesma "escala" para que o modelo n√£o d√™ mais import√¢ncia a uma coluna s√≥ porque seus n√∫meros s√£o maiores.
          * `OneHotEncoder`: Transforma textos em n√∫meros. Por exemplo, a coluna `G√™nero` com valores `Feminino` e `Masculino` vira duas novas colunas, `G√™nero_Feminino` e `G√™nero_Masculino`, com valores `0` ou `1`.
      * **Juntando tudo em um pipeline**:
        ```python
        # Cria um "preparador de dados" que aplica diferentes transforma√ß√µes em diferentes colunas
        pre_processador = ColumnTransformer(
            transformers=[
                ('num', StandardScaler(), colunas_numericas),
                ('cat', OneHotEncoder(), colunas_categoricas)
            ])
        # Cria um "pipeline" que primeiro prepara os dados e depois treina o modelo
        pipeline = Pipeline(steps=[('preprocessor', pre_processador),
                                   ('classifier', RandomForestClassifier())])
        ```
      * **Otimizando o modelo**:
          * `GridSearchCV`: Ajuda a encontrar a melhor "receita" para o modelo, testando v√°rias combina√ß√µes de configura√ß√µes para ver qual d√° o melhor resultado.
      * **Avaliando o desempenho**:
          * `classification_report` e `confusion_matrix`: Depois que o modelo faz as previs√µes, esses m√©todos nos d√£o um relat√≥rio completo sobre o desempenho, mostrando quantos acertos, erros e o desempenho em cada categoria.

  * **Random Forest**: √â um dos modelos de Machine Learning mais populares e eficazes para classifica√ß√£o. Pense nele como uma equipe de muitas "mini-decis√µes" (√°rvores de decis√£o). Cada √°rvore de decis√£o faz sua pr√≥pria previs√£o, e o Random Forest combina as respostas da maioria das √°rvores para chegar a uma previs√£o final, mais robusta e precisa do que uma √∫nica √°rvore.

      * **Treinando o modelo Random Forest**:
        ```python
        # Define o modelo Random Forest
        rf_model = RandomForestClassifier(random_state=42)
        # Cria e treina o pipeline com o modelo
        pipeline = Pipeline(steps=[('preprocessor', pre_processador),
                                   ('classifier', rf_model)])
        pipeline.fit(X_train, y_train)
        ```
      * **Por que o Random Forest √© bom?**: Ele lida bem com dados complexos, √© menos propenso a "overfitting" (quando o modelo decora os dados de treino e n√£o consegue prever novos dados) e a sua estrutura de "floresta" o torna uma ferramenta poderosa para prever o churn.

  * **XGBoost (xgboost)**: Um tipo de modelo de Machine Learning super poderoso e r√°pido, conhecido por ganhar muitas competi√ß√µes de ci√™ncia de dados. √â uma √≥tima alternativa ao Random Forest.

      * **Treinando o modelo XGBoost**:
        ```python
        # Cria e treina um modelo XGBoost dentro do pipeline
        pipeline_xgb = Pipeline(steps=[('preprocessor', pre_processador),
                                       ('classifier', XGBClassifier())])
        pipeline_xgb.fit(X_train, y_train)
        ```

  * **Joblib**: Para salvar o modelo treinado. Pense nisso como "congelar" o modelo para poder us√°-lo depois, sem precisar trein√°-lo novamente.

      * **Salvando e carregando o modelo**:
        ```python
        # Salva o pipeline (o "preparador" + o modelo) em um arquivo
        joblib.dump(pipeline, 'modelo_churn.pkl')
        # Carrega o modelo salvo para usar mais tarde
        modelo_carregado = joblib.load('modelo_churn.pkl')
        ```

## üìà Tratamento e Treinamento dos Dados

O processo de tratamento e treinamento dos dados √© estruturado em um fluxo de trabalho claro, garantindo a qualidade e a reprodutibilidade dos resultados:

1.  **Carregamento e Prepara√ß√£o dos Dados**:

      * Os dados s√£o carregados de um arquivo JSON.
      * As colunas que cont√™m dicion√°rios aninhados s√£o "aplanadas" com `pd.json_normalize()` para que cada informa√ß√£o vire uma coluna separada e f√°cil de usar.
      * A coluna `Churn` √© a nossa "resposta" (o que queremos prever). Ela √© convertida de "Yes"/"No" para `1`/`0`, que s√£o valores que o modelo consegue entender.

2.  **Pr√©-processamento**:

      * Os dados s√£o divididos em `features` (as informa√ß√µes dos clientes) e a vari√°vel alvo (`Churn`).
      * O `train_test_split` separa os dados para treino e teste.
      * O `ColumnTransformer` e o `Pipeline` entram em cena para garantir que os dados sejam preparados (normalizados e codificados) de forma consistente antes de serem enviados para o modelo.

3.  **Treinamento e Avalia√ß√£o dos Modelos**:

      * S√£o treinados dois modelos: Random Forest e XGBoost.
      * O `GridSearchCV` √© usado para encontrar os melhores "ajustes" para o Random Forest, garantindo que ele tenha a melhor performance poss√≠vel.
      * Por fim, o desempenho √© avaliado com o `classification_report` e a `confusion_matrix`, para sabermos qu√£o bem o modelo est√° prevendo o churn.

### **Por que usar Random Forest e XGBoost em conjunto?**

No mundo do Machine Learning, √© comum n√£o se contentar com apenas um modelo. A melhor pr√°tica √© treinar e comparar diferentes modelos para ver qual deles se adapta melhor aos seus dados e ao seu problema. √â por isso que o projeto utiliza tanto o **Random Forest** quanto o **XGBoost**.

Ambos s√£o modelos de "ensemble", ou seja, eles combinam a for√ßa de v√°rias √°rvores de decis√£o para criar uma previs√£o final mais precisa. No entanto, eles fazem isso de maneiras diferentes:

  * **Random Forest**: √â um m√©todo de "bagging" (do ingl√™s, *bootstrap aggregating*). Ele treina v√°rias √°rvores de forma independente em diferentes subconjuntos de dados. A previs√£o final √© a m√©dia ou a vota√ß√£o da maioria das √°rvores. √â um modelo robusto, f√°cil de paralelizar (o que o torna r√°pido em m√°quinas com v√°rios n√∫cleos) e menos propenso a overfitting.

  * **XGBoost**: √â um m√©todo de "boosting" (do ingl√™s, *gradiente boosting*). Ele treina as √°rvores de forma sequencial, onde cada nova √°rvore √© criada para corrigir os erros cometidos pelas √°rvores anteriores. Isso permite que o modelo aprenda com seus pr√≥prios erros e se torne progressivamente mais preciso. √â conhecido por sua alta performance e por ser um dos modelos mais utilizados em competi√ß√µes de ci√™ncia de dados.

Ao usar os dois modelos no mesmo projeto, o desenvolvedor pode **comparar seus desempenhos**, analisando m√©tricas como precis√£o, recall e F1-score no conjunto de teste. O objetivo √© escolher o modelo que oferece o melhor equil√≠brio entre precis√£o e interpretabilidade para a tarefa de prever o churn.

### **Entendendo as M√©tricas de Avalia√ß√£o: Precis√£o, Recall e F1-score**

Depois que o modelo faz as previs√µes, √© crucial entender se ele est√° fazendo um bom trabalho. Para isso, utilizamos as seguintes m√©tricas, que s√£o calculadas a partir da matriz de confus√£o:

  * **Precis√£o (Precision)**: Responde √† pergunta: **"Dos clientes que o modelo previu que iriam 'churnar', quantos realmente churnaram?"**

      * √â uma m√©trica importante quando o custo de um falso positivo √© alto. Por exemplo, se a empresa gasta muito dinheiro em uma campanha de reten√ß√£o para clientes que n√£o iriam embora, uma alta precis√£o √© fundamental.
      * **F√≥rmula**: `Precis√£o = (Verdadeiros Positivos) / (Verdadeiros Positivos + Falsos Positivos)`
      * **Exemplo**: Se o modelo previu 100 clientes com churn e, na verdade, 80 deles realmente cancelaram, a precis√£o √© de 80%. Isso significa que, quando o modelo "dispara o alarme", ele est√° certo em 80% das vezes.

  * **Recall (Sensibilidade)**: Responde √† pergunta: **"Dos clientes que realmente churnaram, quantos o modelo conseguiu identificar?"**

      * √â crucial quando o custo de um falso negativo √© alto. No nosso caso, um falso negativo √© um cliente que o modelo previu que ficaria, mas ele acabou cancelando. A empresa quer evitar ao m√°ximo perder clientes, ent√£o um alto recall √© muito importante para capturar a maior quantidade poss√≠vel de clientes em risco.
      * **F√≥rmula**: `Recall = (Verdadeiros Positivos) / (Verdadeiros Positivos + Falsos Negativos)`
      * **Exemplo**: Se 120 clientes realmente churnaram, mas o modelo s√≥ identificou 80 deles, o recall √© de aproximadamente 66%. Isso significa que o modelo "perdeu" 40 clientes que estavam em risco.

  * **F1-score**: √â uma m√©dia harm√¥nica entre a precis√£o e o recall. Ele busca o equil√≠brio entre as duas m√©tricas.

      * O F1-score √© particularmente √∫til quando as classes est√£o desbalanceadas (existem muito mais clientes que n√£o churnam do que clientes que churnam). Ele penaliza modelos que favorecem uma m√©trica em detrimento da outra.
      * **Exemplo**: Se um modelo tem precis√£o de 95% mas recall de apenas 10%, ele √© muito preciso, mas n√£o consegue identificar a maioria dos clientes em risco. O F1-score seria baixo, mostrando que o modelo n√£o √© bom no geral. Um bom F1-score indica que o modelo tem um bom equil√≠brio entre capturar a maioria dos clientes em risco e ser preciso em suas previs√µes.

### **Como testamos o modelo com dados de mentira?**

Depois que o modelo est√° treinado, ele precisa ser testado com dados novos, como se um cliente real estivesse ligando. Para isso, o notebook usa dados fict√≠cios.

1.  **Cria√ß√£o dos Dados Fict√≠cios**:
      * O notebook cria um novo `DataFrame` com informa√ß√µes de clientes que n√£o foram usadas no treinamento.
    <!-- end list -->
    ```python
    # Cria um novo DataFrame com dados de exemplo
    df_novo_cliente = pd.DataFrame([
      {'customer_gender': 'Male', 'customer_SeniorCitizen': 'No', ...},
      {'customer_gender': 'Female', 'customer_SeniorCitizen': 'Yes', ...}
    ])
    ```
2.  **Fazendo a Predi√ß√£o**:
      * O pipeline, que j√° foi salvo e carregado, √© usado para fazer a previs√£o. Ele recebe os dados de mentira e, magicamente, aplica toda a prepara√ß√£o dos dados e o modelo em um √∫nico passo.
    <!-- end list -->
    ```python
    # Carrega o modelo treinado
    modelo_carregado = joblib.load('modelo_churn.pkl')
    # Faz a previs√£o de churn para os novos clientes
    previsoes = modelo_carregado.predict(df_novo_cliente)
    ```
3.  **Interpretando o Resultado**:
      * O resultado √© um array com `0`s e `1`s. Um `1` significa que o modelo acha que o cliente vai cancelar, e um `0` significa que ele deve ficar. Essa previs√£o √© adicionada de volta ao `DataFrame` para que a equipe de neg√≥cios possa ver quais clientes t√™m maior risco e tomar uma atitude.

### **Por que a coluna `Churn` √© t√£o importante?**

A coluna `Churn` √© a **vari√°vel-alvo**, a "resposta" que o modelo precisa aprender a adivinhar.

  * **Para o Treinamento**: A coluna `Churn` √© usada como o "gabarito" do nosso modelo. O algoritmo olha para todas as caracter√≠sticas dos clientes (idade, tipo de contrato, etc.) e a associa com o resultado `Churn` (`1` para cancelou, `0` para n√£o cancelou). √â assim que ele aprende quais padr√µes levam um cliente a ir embora.
  * **Para a Predi√ß√£o**: Quando vamos prever o churn de um cliente novo, essa coluna n√£o existe. O modelo, com base em tudo o que aprendeu, usa as outras informa√ß√µes do cliente para dar uma resposta (`1` ou `0`). Se a coluna `Churn` j√° estivesse l√°, n√£o precisar√≠amos de um modelo\!

Em resumo, a coluna `Churn` √© necess√°ria para ensinar o modelo, mas n√£o √© usada para fazer as previs√µes, pois √© o que estamos tentando descobrir.


## ü§ù Contato

Se voc√™ tiver d√∫vidas, sugest√µes ou quiser discutir o projeto, sinta-se √† vontade para entrar em contato:

  * **Nome:** Luciano Azevedo (Especialista Data Science)
  * **Email:** lucianocomputador@gmail.com
  * **LinkedIn:** [Meu Perfil LinkedIn](https://www.linkedin.com/in/luciano-devops/)
  * **GitHub:** [Meu Perfil GitHub](https://github.com/lfa-systems)


-----

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a licen√ßa **MIT License** - veja o arquivo [LICENSE](https://www.google.com/search?q=MIT+License&oq=MIT+License) para detalhes.

-----